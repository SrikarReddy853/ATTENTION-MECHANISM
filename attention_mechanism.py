# -*- coding: utf-8 -*-
"""ATTENTION MECHANISM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gD1yn6jRMHJiCf0e5vGcoIrC7XUAgA-T
"""

# ============================================================
# LAB -4 EXPERIMENT: UNDERSTANDING ATTENTION MECHANISM
# ============================================================
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
# Import PyTorch for tensor operations
# Import functional API for softmax
# Import matplotlib for visualization
# ------------------------------------------------------------
# STEP 1: INPUT SENTENCE (WORDS)
# ------------------------------------------------------------
# Students can change this sentence to test different inputs
words = ["I", "love", "AI"]
print("Input Words:", words)
# ------------------------------------------------------------
# STEP 2: CONVERT WORDS TO ENCODER OUTPUTS (TENSORS)
# ------------------------------------------------------------
# In real scenarios, these would come from an embedding layer or encoder
# Here we manually assign a 2-dimensional vector for simplicity
encoder_outputs = torch.tensor([
[0.1, 0.3],   # Representation for "I"
[0.8, 0.2],   # Representation for "love"
[0.9, 0.7]
# Representation for "AI"
], dtype=torch.float)
print("\nEncoder Outputs (Keys & Values):")
print(encoder_outputs)   # Display tensor representations
# ------------------------------------------------------------
# STEP 3: DECODER HIDDEN STATE (QUERY)
# ------------------------------------------------------------
# The decoder hidden state represents the current decoding step
# This acts as the "Query" to ask the encoder what to focus on
decoder_hidden = torch.tensor([0.7, 0.6], dtype=torch.float)
print("\nDecoder Hidden State (Query):", decoder_hidden)
# ------------------------------------------------------------
# STEP 4: COMPUTE ATTENTION SCORES
# ------------------------------------------------------------
# Attention score = dot product between Query and each Key (encoder
# outputs)
# Higher score = more relevant word
attention_scores = torch.matmul(encoder_outputs, decoder_hidden)
# Convert scores to probabilities (softmax) so they sum to 1
attention_weights = F.softmax(attention_scores, dim=0)
print("\nAttention Weights (Original Query):", attention_weights)
# ------------------------------------------------------------
# STEP 5: COMPUTE CONTEXT VECTOR
# ------------------------------------------------------------
# Context vector = weighted sum of encoder outputs
# This vector represents the combined information the decoder should focus
# on
context_vector = torch.sum(attention_weights.unsqueeze(1) *
encoder_outputs, dim=0)
print("\nContext Vector:", context_vector)
# ------------------------------------------------------------
# STEP 6: VISUALIZE ORIGINAL ATTENTION WEIGHTS
# ------------------------------------------------------------
weights_np = attention_weights.detach().numpy()  # Convert tensor to NumPy
# for plotting
plt.figure(figsize=(6, 4))
plt.bar(words, weights_np)
# weights
# Bar chart of attention
plt.title("Original Attention Weight Distribution")
plt.xlabel("Input Words")
plt.ylabel("Attention Weight")
plt.show()
# ------------------------------------------------------------
# STEP 7: CHANGE DECODER HIDDEN STATE TO SEE ATTENTION SHIFT
# ------------------------------------------------------------
# Simulate a different decoder query to see how attention changes
decoder_hidden_new = torch.tensor([0.2, 0.9], dtype=torch.float)
new_scores = torch.matmul(encoder_outputs, decoder_hidden_new)
new_weights = F.softmax(new_scores, dim=0)
print("\nNew Decoder Hidden State:", decoder_hidden_new)
print("New Attention Weights:", new_weights)
# ------------------------------------------------------------
# STEP 8: VISUALIZE ATTENTION SHIFT WITH COMPARISON
# ------------------------------------------------------------
new_weights_np = new_weights.detach().numpy()
# Plot original and new attention weights together
plt.figure(figsize=(8, 4))
plt.bar(words, weights_np, alpha=0.6, label='Original Query')
plt.bar(words, new_weights_np, alpha=0.6, label='New Query')
plt.title("Attention Shift Between Queries")
plt.xlabel("Input Words")
plt.ylabel("Attention Weight")
plt.legend()
plt.show()
# Optional: Draw arrows to show how attention shifts visually
plt.figure(figsize=(8, 4))
for i, (w_old, w_new) in enumerate(zip(weights_np, new_weights_np)):
    plt.arrow(i, w_old, 0, w_new - w_old, head_width=0.1,
              head_length=0.02, color='red')
    plt.bar(words, new_weights_np, alpha=0.6)
plt.title("Attention Shift Arrows (Red)")
plt.xlabel("Input Words")
plt.ylabel("Attention Weight")
plt.show()

#lab-3
# Tiny / Fast Seq2Seq EN->FR (single cell). Paste into one Colab cell.
# Fast demo: small data, small model, 1-2 epochs.
import os, random, numpy as np, pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

# ----------------- User-tunable (fast) -----------------
MAX_SAMPLES = 2000    # use 2000 pairs for quick run
num_words_en = 8000
num_words_fr = 8000
embedding_dim = 64
latent_dim = 128
epochs = 2            # set 1 for even faster run
batch_size = 128
# ------------------------------------------------------

# Download dataset if not present
if not os.path.exists("fra.txt") and not os.path.exists("fra-eng/fra.txt"):
    !wget -q -O fra-eng.zip "http://www.manythings.org/anki/fra-eng.zip" || true
    !unzip -qq fra-eng.zip || true

# Find file
data_path = "fra.txt" if os.path.exists("fra.txt") else "fra-eng/fra.txt"
if not os.path.exists(data_path):
    raise FileNotFoundError("fra.txt not found. Upload or re-run download step.")

# Load pairs
with open(data_path, encoding="utf-8") as f:
    lines = [ln for ln in f.read().splitlines() if "\t" in ln]
pairs = []
for ln in lines:
    en, fr = ln.split("\t")[:2]
    pairs.append((en.strip(), fr.strip()))
random.shuffle(pairs)
pairs = pairs[:min(MAX_SAMPLES, len(pairs))]
print(f"Using {len(pairs)} sentence pairs; epochs={epochs}")

# Prepare texts
input_texts, target_in_texts, target_out_texts = [], [], []
for en, fr in pairs:
    input_texts.append(en)
    target_in_texts.append("<start> " + fr)
    target_out_texts.append(fr + " <end>")

# Tokenize
eng_tok = Tokenizer(num_words=num_words_en, filters="", oov_token="<unk>")
fra_tok = Tokenizer(num_words=num_words_fr, filters="", oov_token="<unk>")
eng_tok.fit_on_texts(input_texts)
fra_tok.fit_on_texts(target_in_texts + target_out_texts)

enc_seq = eng_tok.texts_to_sequences(input_texts)
dec_in_seq = fra_tok.texts_to_sequences(target_in_texts)
dec_out_seq = fra_tok.texts_to_sequences(target_out_texts)

max_enc = max(len(s) for s in enc_seq)
max_dec = max(len(s) for s in dec_in_seq)

enc_input = pad_sequences(enc_seq, maxlen=max_enc, padding="post")
dec_input = pad_sequences(dec_in_seq, maxlen=max_dec, padding="post")
dec_output = pad_sequences(dec_out_seq, maxlen=max_dec, padding="post")
dec_output = np.expand_dims(dec_output, -1)

vocab_en = min(num_words_en, len(eng_tok.word_index) + 1)
vocab_fr = min(num_words_fr, len(fra_tok.word_index) + 1)

# Build small Seq2Seq model
enc_inputs = Input(shape=(None,))
enc_emb = Embedding(vocab_en, embedding_dim, mask_zero=True)(enc_inputs)
enc_lstm = LSTM(latent_dim, return_state=True)
_, h, c = enc_lstm(enc_emb)
states = [h, c]

dec_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(vocab_fr, embedding_dim, mask_zero=True)
dec_emb = dec_emb_layer(dec_inputs)
dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
dec_out, _, _ = dec_lstm(dec_emb, initial_state=states)
dec_dense = Dense(vocab_fr, activation="softmax")
dec_out = dec_dense(dec_out)

model = Model([enc_inputs, dec_inputs], dec_out)
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")
print("Model built. Starting quick training...")

# Train (quick)
model.fit([enc_input, dec_input], dec_output,
          batch_size=batch_size, epochs=epochs,
          validation_split=0.1, verbose=1)

# Build inference models
encoder_model = Model(enc_inputs, states)

dec_state_h = Input(shape=(latent_dim,))
dec_state_c = Input(shape=(latent_dim,))
dec_states_inputs = [dec_state_h, dec_state_c]
dec_emb2 = dec_emb_layer(dec_inputs)
dec_out2, out_h2, out_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inputs)
dec_out2 = dec_dense(dec_out2)
decoder_model = Model([dec_inputs] + dec_states_inputs, [dec_out2, out_h2, out_c2])

rev_fra = {i: w for w, i in fra_tok.word_index.items()}

def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq, verbose=0)
    start_idx = fra_tok.word_index.get("<start>")
    end_idx = fra_tok.word_index.get("<end>")
    if start_idx is None or end_idx is None:
        return ""
    target_seq = np.array([[start_idx]])
    h, c = states_value
    decoded = []
    for _ in range(max_dec):
        output_tokens, h, c = decoder_model.predict([target_seq, h, c], verbose=0)
        sampled_idx = int(np.argmax(output_tokens[0, -1, :]))
        if sampled_idx == end_idx:
            break
        decoded.append(rev_fra.get(sampled_idx, ""))
        target_seq = np.array([[sampled_idx]])
    return " ".join(decoded)

def translate(text):
    seq = eng_tok.texts_to_sequences([text])
    seq = pad_sequences(seq, maxlen=max_enc, padding="post")
    return decode_sequence(seq)

# Show a few quick examples
print("\n--- Quick sample translations ---")
for i in range(5):
    en, fr = pairs[i]
    print("EN:", en)
    print("TRUE:", fr)
    print("PRED:", translate(en))
    print("------")

print("Done. To get even faster run set: MAX_SAMPLES=500, epochs=1, latent_dim=64.")

#lab-2
import math
import numpy as np
from collections import defaultdict, Counter
from typing import List, Tuple

# -------------------------- Preprocessing --------------------------
def tokenize(sentence: str) -> List[str]:
    tokens = []
    current = ""
    for ch in sentence:
        if ch.isalnum() or ch == "'":
            current += ch.lower()
        else:
            if current:
                tokens.append(current)
                current = ""
            if ch.strip():
                tokens.append(ch)
    if current:
        tokens.append(current)
    return tokens

def prepare_corpus(sentences, start_token="<s>", end_token="</s>"):
    return [[start_token] + tokenize(s) + [end_token] for s in sentences]

# -------------------------- N-gram model --------------------------
class NGramModel:
    def __init__(self, n=2, smoothing=1.0):
        self.n = n
        self.smoothing = smoothing
        self.counts = defaultdict(Counter)
        self.context_totals = Counter()
        self.vocab = set()

    def train(self, corpus):
        for sent in corpus:
            padded = ["<PAD>"] * (self.n - 1) + sent
            for i in range(self.n - 1, len(padded)):
                context = tuple(padded[i - (self.n - 1):i])
                token = padded[i]
                self.counts[context][token] += 1
                self.context_totals[context] += 1
                self.vocab.add(token)
        self.vocab.add("<PAD>")

    def prob(self, context, token):
        c = self.counts[context][token]
        total = self.context_totals[context]
        V = len(self.vocab)
        return (c + self.smoothing) / (total + self.smoothing * V)

    def sentence_logprob(self, sentence):
        padded = ["<PAD>"] * (self.n - 1) + sentence
        logprob = 0.0
        for i in range(self.n - 1, len(padded)):
            context = tuple(padded[i - (self.n - 1):i])
            logprob += math.log(self.prob(context, padded[i]))
        return logprob

    def perplexity(self, corpus):
        total_logprob = 0
        N = 0
        for s in corpus:
            total_logprob += self.sentence_logprob(s)
            N += len(s)
        return math.exp(-total_logprob / N)

# -------------------------- BLEU --------------------------
def ngram_counts(tokens, n):
    c = Counter()
    for i in range(len(tokens) - n + 1):
        c[tuple(tokens[i:i+n])] += 1
    return c

def clipped_precision(candidate, references, n):
    cand = ngram_counts(candidate, n)
    max_ref = Counter()
    for ref in references:
        rc = ngram_counts(ref, n)
        for ng in rc:
            max_ref[ng] = max(max_ref[ng], rc[ng])
    clipped = {ng: min(count, max_ref.get(ng, 0)) for ng, count in cand.items()}
    return sum(clipped.values()), sum(cand.values())

def brevity_penalty(cand, refs):
    c = len(cand)
    ref_lens = [len(r) for r in refs]
    best_ref = min(ref_lens, key=lambda rl: (abs(rl - c), rl))
    if c == 0:
        return 0.0
    if c > best_ref:
        return 1.0
    return math.exp(1 - best_ref / c)

def sentence_bleu(candidate, references, max_n=4):
    p_ns = []
    for n in range(1, max_n + 1):
        num, den = clipped_precision(candidate, references, n)
        p = num / den if den > 0 else 0
        p_ns.append(p)
    if min(p_ns) == 0:
        geo_mean = 0
    else:
        geo_mean = math.exp(sum((1/max_n)*math.log(p) for p in p_ns))
    return brevity_penalty(candidate, references) * geo_mean

# -------------------------- Tiny RNN LM --------------------------
class TinyRNNLM:
    def __init__(self, vocab, hidden_size=32, learning_rate=0.5):
        self.vocab = vocab
        self.word2idx = {w:i for i,w in enumerate(vocab)}
        self.idx2word = {i:w for w,i in self.word2idx.items()}
        V = len(vocab)
        H = hidden_size

        self.Wxh = np.random.randn(H, V) * 0.01
        self.Whh = np.random.randn(H, H) * 0.01
        self.bh  = np.zeros((H,1))
        self.Why = np.random.randn(V, H) * 0.01
        self.by  = np.zeros((V,1))

        self.V = V
        self.H = H
        self.lr = learning_rate

    def one_hot(self, idx):
        x = np.zeros((self.V,1))
        x[idx] = 1
        return x

    def forward(self, seq):
        hs, ps = {}, {}
        hs[-1] = np.zeros((self.H,1))
        for t, idx in enumerate(seq):
            x = self.one_hot(idx)
            hs[t] = np.tanh(self.Wxh @ x + self.Whh @ hs[t-1] + self.bh)
            y = self.Why @ hs[t] + self.by
            p = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))
            ps[t] = p
        return hs, ps

    def loss(self, seq):
        _, ps = self.forward(seq[:-1])
        L = 0
        for t,p in ps.items():
            L -= math.log(p[seq[t+1],0] + 1e-12)
        return L

    def perplexity(self, corpus):
        total_loss = 0
        N = 0
        for seq in corpus:
            total_loss += self.loss(seq)
            N += len(seq) - 1
        return math.exp(total_loss / N)

# -------------------------- Report Generator --------------------------
def generate_report(train, test, bigram_ppl, trigram_ppl, bleu_score, rnn_history):

    print("\n" + "="*70)
    print("                     MACHINE LEARNING NLP REPORT")
    print("="*70)

    print("\n1. TRAINING SENTENCES (Tokenized Corpus):")
    print("-"*70)
    for i, s in enumerate(train, 1):
        print(f"{i:2d}. {' '.join(s)}")

    print("\n2. TEST SENTENCES:")
    print("-"*70)
    for i, s in enumerate(test, 1):
        print(f"{i:2d}. {' '.join(s)}")

    print("\n3. N-GRAM PERPLEXITIES:")
    print("-"*70)
    print(f"Bigram Perplexity  : {bigram_ppl:.4f}")
    print(f"Trigram Perplexity : {trigram_ppl:.4f}")

    print("\n4. BLEU SCORE:")
    print("-"*70)
    print(f"BLEU Score: {bleu_score:.4f}")

    print("\n5. RNN PERPLEXITY OVER EPOCHS:")
    print("-"*70)
    for i, p in enumerate(rnn_history, 1):
        print(f"Epoch {i}: Test Perplexity = {p:.4f}")

    print("\n" + "="*70)
    print("                            END OF REPORT")
    print("="*70 + "\n")

# ------------------------------ MAIN ------------------------------
if __name__ == "__main__":

    # Raw corpus
    raw_train = [
        "I like natural language processing .",
        "I like deep learning .",
        "I enjoy learning .",
        "natural language processing is fun ."
    ]
    raw_test = [
        "I like learning .",
        "natural language is fun ."
    ]

    train = prepare_corpus(raw_train)
    test = prepare_corpus(raw_test)

    # N-gram models
    bigram = NGramModel(2, 0.5);  bigram.train(train)
    trigram = NGramModel(3, 0.5); trigram.train(train)

    bigram_ppl = bigram.perplexity(test)
    trigram_ppl = trigram.perplexity(test)

    # BLEU
    cand = tokenize("I like learning .")
    refs = [
        tokenize("I like deep learning ."),
        tokenize("I enjoy learning .")
    ]
    bleu = sentence_bleu(cand, refs)

    # RNN LM
    vocab = sorted({w for s in train for w in s})
    word2idx = {w:i for i,w in enumerate(vocab)}

    train_idx = [[word2idx[w] for w in s] for s in train]
    test_idx = [[word2idx[w] for w in s] for s in test]

    rnn = TinyRNNLM(vocab, hidden_size=32, learning_rate=0.5)

    rnn_history = []
    for epoch in range(5):
        ppl = rnn.perplexity(test_idx)
        rnn_history.append(ppl)

    # Generate final report
    generate_report(train, test, bigram_ppl, trigram_ppl, bleu, rnn_history)

#lab-1
# Simple Seq2Seq (Encoder-Decoder) English->French (short & runnable)
# If needed in Colab: uncomment
# !pip install --quiet torch nltk

import random, math
import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ----- tiny toy parallel dataset (english, french) -----
pairs = [
    ("i am cold", "je ai froid"),
    ("he is hungry", "il a faim"),
    ("she loves me", "elle m aime"),
    ("i like cats", "j aime les chats"),
    ("we are friends", "nous sommes amis"),
    ("thank you", "merci"),
    ("good morning", "bonjour"),
    ("how are you", "comment allez vous"),
    ("see you", "a plus tard"),
    ("i love you", "je t aime")
]
random.shuffle(pairs)

# ----- tokenization & vocab helpers (very simple split) -----
SRC_PAD, TGT_PAD = "<pad>", "<pad>"
SRC_BOS, TGT_BOS = "<sos>", "<sos>"
SRC_EOS, TGT_EOS = "<eos>", "<eos>"

def build_vocab(sentences, specials=[SRC_PAD, SRC_BOS, SRC_EOS]):
    tok = {}
    idx = 0
    for s in specials:
        tok[s] = idx; idx += 1
    for sent in sentences:
        for w in sent.strip().split():
            if w not in tok:
                tok[w] = idx; idx += 1
    itos = {i:w for w,i in tok.items()}
    return tok, itos

src_sents = [s for s,t in pairs]
tgt_sents = [t for s,t in pairs]
src_vocab, src_itos = build_vocab(src_sents, specials=[SRC_PAD, SRC_BOS, SRC_EOS])
tgt_vocab, tgt_itos = build_vocab(tgt_sents, specials=[TGT_PAD, TGT_BOS, TGT_EOS])

def encode(sent, vocab, add_bos=True, add_eos=True):
    toks = sent.strip().split()
    ids = []
    if add_bos: ids.append(vocab[SRC_BOS] if SRC_BOS in vocab else vocab[TGT_BOS])
    for w in toks:
        ids.append(vocab.get(w, vocab[vocab.keys().__iter__().__next__()]))  # fall back to first token
    if add_eos: ids.append(vocab[SRC_EOS] if SRC_EOS in vocab else vocab[TGT_EOS])
    return ids

# ----- Dataset & collate -----
class ParallelDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs
    def __len__(self): return len(self.pairs)
    def __getitem__(self, idx):
        src, trg = self.pairs[idx]
        return encode(src, src_vocab), encode(trg, tgt_vocab)

def collate(batch):
    srcs, trgs = zip(*batch)
    max_s = max(len(x) for x in srcs)
    max_t = max(len(x) for x in trgs)
    src_tensor = torch.full((len(srcs), max_s), src_vocab[SRC_PAD], dtype=torch.long)
    trg_tensor = torch.full((len(trgs), max_t), tgt_vocab[TGT_PAD], dtype=torch.long)
    for i,x in enumerate(srcs): src_tensor[i, :len(x)] = torch.tensor(x)
    for i,y in enumerate(trgs): trg_tensor[i, :len(y)] = torch.tensor(y)
    return src_tensor, trg_tensor

dl = DataLoader(ParallelDataset(pairs), batch_size=2, shuffle=True, collate_fn=collate)

# ----- Encoder & Decoder (LSTM) -----
class Encoder(nn.Module):
    def __init__(self, vocab_size, emb_size=64, hid=128):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, emb_size, padding_idx=0)
        self.lstm = nn.LSTM(emb_size, hid, batch_first=True)
    def forward(self, x):
        e = self.embed(x)
        out, (h,c) = self.lstm(e)
        return h, c

class Decoder(nn.Module):
    def __init__(self, vocab_size, emb_size=64, hid=128):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, emb_size, padding_idx=0)
        self.lstm = nn.LSTM(emb_size, hid, batch_first=True)
        self.fc = nn.Linear(hid, vocab_size)
    def forward(self, x, hidden):
        # x: [B, 1]
        e = self.embed(x)
        out, hidden = self.lstm(e, hidden)
        logits = self.fc(out.squeeze(1))  # [B, V]
        return logits, hidden

# ----- Seq2seq wrapper -----
class Seq2Seq(nn.Module):
    def __init__(self, enc, dec, device='cpu'):
        super().__init__()
        self.enc = enc; self.dec = dec; self.device = device
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.size(0)
        max_t = trg.size(1)
        vocab_size = self.dec.fc.out_features
        outputs = torch.zeros(batch_size, max_t, vocab_size).to(self.device)
        h, c = self.enc(src)
        input_tok = trg[:,0].unsqueeze(1)  # first token (should be <sos>)
        hidden = (h,c)
        for t in range(1, max_t):
            logits, hidden = self.dec(input_tok, hidden)
            outputs[:, t] = logits
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = logits.argmax(1).unsqueeze(1)
            input_tok = (trg[:, t].unsqueeze(1) if teacher_force else top1)
        return outputs

# ----- Instantiate model, loss, optimizer -----
device = 'cuda' if torch.cuda.is_available() else 'cpu'
enc = Encoder(len(src_vocab)).to(device)
dec = Decoder(len(tgt_vocab)).to(device)
model = Seq2Seq(enc, dec, device).to(device)
criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[TGT_PAD])
opt = torch.optim.Adam(model.parameters(), lr=0.01)

# ----- training (very small epochs because dataset tiny) -----
for epoch in range(200):
    model.train()
    total = 0.0
    for src, trg in dl:
        src, trg = src.to(device), trg.to(device)
        opt.zero_grad()
        out = model(src, trg, teacher_forcing_ratio=0.7)
        # shift for loss: predict t from t-1 -> compare outputs[:,1:] with trg[:,1:]
        logits = out[:,1:,:].reshape(-1, out.size(-1))
        targets = trg[:,1:].reshape(-1)
        loss = criterion(logits, targets)
        loss.backward(); opt.step()
        total += loss.item()
    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1} loss={total/len(dl):.4f}")

# ----- inference / translate -----
def translate(model, sentence, max_len=12):
    model.eval()
    src_ids = torch.tensor([encode(sentence, src_vocab)], dtype=torch.long).to(device)
    with torch.no_grad():
        h,c = model.enc(src_ids)
        input_tok = torch.tensor([[tgt_vocab[TGT_BOS]]], dtype=torch.long).to(device)
        hidden = (h,c)
        words=[]
        for _ in range(max_len):
            logits, hidden = model.dec(input_tok, hidden)
            next_id = logits.argmax(1).item()
            if next_id == tgt_vocab[TGT_EOS]: break
            words.append(tgt_itos[next_id])
            input_tok = torch.tensor([[next_id]], dtype=torch.long).to(device)
    return " ".join(words)

# ----- Try translating toy sentences -----
tests = ["i am cold", "thank you", "we are friends", "i love you"]
for s in tests:
    print(f"EN: {s}  ->  FR: {translate(model, s)}")